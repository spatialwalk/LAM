#!/usr/bin/env python3
"""
æ•´åˆçš„å¤´åƒç”Ÿæˆç³»ç»Ÿ
ç»“åˆäº†VHAPæ•°æ®è½¬æ¢å’Œå¤šè§†è§’å¤´åƒè§†é¢‘ç”ŸæˆåŠŸèƒ½

åŠŸèƒ½ç‰¹æ€§ï¼š
1. è‡ªåŠ¨è½¬æ¢VHAPè¾“å‡ºä¸ºLAMå…¼å®¹æ ¼å¼
2. æ”¯æŒå¤šç§è§†é¢‘æ ¼å¼è¾“å…¥å¤„ç†
3. ç”Ÿæˆå¤šä¸ªä¸åŒè§†è§’çš„å¤´åƒåŠ¨ç”»è§†é¢‘
4. è‡ªåŠ¨éŸ³é¢‘æå–å’ŒåŒæ­¥
5. å®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹

ä½¿ç”¨æ–¹æ³•ï¼š
python integrated_avatar_generation.py --input_image path/to/image.jpg --source_video path/to/video.mp4
"""

import os
import cv2
import sys
import base64
import subprocess
import numpy as np
from PIL import Image
import argparse
from omegaconf import OmegaConf
import torch
import zipfile
from glob import glob
import moviepy.editor as mpy
from pathlib import Path
import json
import shutil
from tqdm import tqdm

# LAMç›¸å…³å¯¼å…¥
from lam.utils.ffmpeg_utils import images_to_video
from tools.flame_tracking_single_image import FlameTrackingSingleImage
from lam.runners.infer.head_utils import prepare_motion_seqs, preprocess_image

try:
    import spaces
except:
    pass

# å…¨å±€é…ç½®
h5_rendering = False

class IntegratedAvatarGenerator:
    """æ•´åˆçš„å¤´åƒç”Ÿæˆå™¨ç±»"""
    
    def __init__(self, config):
        self.config = config
        self.lam_model = None
        self.flametracking = None
        self.preprocessor = None
        
    def init_preprocessor(self):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        from lam.utils.preprocess import Preprocessor
        self.preprocessor = Preprocessor()
        print("âœ… é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def init_flame_tracking(self):
        """åˆå§‹åŒ–FLAMEè·Ÿè¸ªå™¨"""
        self.flametracking = FlameTrackingSingleImage(
            output_dir='tracking_output',
            alignment_model_path='./model_zoo/flame_tracking_models/68_keypoints_model.pkl',
            vgghead_model_path='./model_zoo/flame_tracking_models/vgghead/vgg_heads_l.trcd',
            human_matting_path='./model_zoo/flame_tracking_models/matting/stylematte_synth.pt',
            facebox_model_path='./model_zoo/flame_tracking_models/FaceBoxesV2.pth',
            detect_iris_landmarks=True
        )
        print("âœ… FLAMEè·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ")
        
    def init_lam_model(self):
        """åˆå§‹åŒ–LAMæ¨¡å‹"""
        from lam.models import model_dict
        from lam.utils.hf_hub import wrap_model_hub
        
        hf_model_cls = wrap_model_hub(model_dict["lam"])
        self.lam_model = hf_model_cls.from_pretrained(self.config.model_name)
        self.lam_model.to('cuda')
        print("âœ… LAMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
    def get_video_info(self, video_path):
        """è·å–è§†é¢‘æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        try:
            from moviepy.editor import VideoFileClip
            clip = VideoFileClip(video_path)
            
            info = {
                'duration': clip.duration,
                'fps': clip.fps,
                'size': clip.size,
                'has_audio': clip.audio is not None,
                'format': os.path.splitext(video_path)[1].lower()
            }
            
            clip.close()
            return info
        except Exception as e:
            print(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
            
    def convert_video_format(self, input_video_path, output_video_path, target_fps=30):
        """è½¬æ¢è§†é¢‘æ ¼å¼å¹¶æå–éŸ³é¢‘"""
        try:
            from moviepy.editor import VideoFileClip
            print(f"ğŸ¬ æ­£åœ¨è½¬æ¢è§†é¢‘æ ¼å¼: {input_video_path} -> {output_video_path}")
            
            os.makedirs(os.path.dirname(output_video_path), exist_ok=True)
            
            clip = VideoFileClip(input_video_path)
            print(f"   åŸå§‹è§†é¢‘ä¿¡æ¯: {clip.duration:.2f}ç§’, {clip.fps:.2f}FPS, {clip.size[0]}x{clip.size[1]}")
            
            if clip.size[1] > 720:
                clip_resized = clip.resize(height=720)
                print(f"   è°ƒæ•´åˆ†è¾¨ç‡ä¸º: {clip_resized.size[0]}x{clip_resized.size[1]}")
            else:
                clip_resized = clip
            
            clip_resized.write_videofile(
                output_video_path, 
                codec='libx264', 
                fps=target_fps,
                audio_codec='aac',
                verbose=False,
                logger=None
            )
            
            audio_path = output_video_path.replace('.mp4', '.wav')
            audio_success = False
            
            if clip.audio is not None:
                try:
                    print(f"ğŸµ æ­£åœ¨æå–éŸ³é¢‘...")
                    clip.audio.write_audiofile(
                        audio_path,
                        verbose=False,
                        logger=None
                    )
                    if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                        audio_size = os.path.getsize(audio_path) / 1024
                        print(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_path} ({audio_size:.1f} KB)")
                        audio_success = True
                    else:
                        print("âš ï¸  éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©º")
                        audio_path = None
                except Exception as audio_error:
                    print(f"âš ï¸  éŸ³é¢‘æå–å¤±è´¥: {str(audio_error)}")
                    audio_path = None
            else:
                print("âš ï¸  æºè§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“")
                audio_path = None
            
            if clip.audio:
                clip.audio.close()
            clip.close()
            if clip_resized != clip:
                clip_resized.close()
            
            video_size = os.path.getsize(output_video_path) / 1024 / 1024
            print(f"âœ… è§†é¢‘è½¬æ¢æˆåŠŸ: {output_video_path} ({video_size:.1f} MB)")
            return True, audio_path
            
        except Exception as e:
            print(f"âŒ è§†é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
            return False, None
            
    def process_video_file(self, source_video_path, output_dir, target_name):
        """å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œå¤åˆ¶/è½¬æ¢è§†é¢‘å¹¶æå–éŸ³é¢‘"""
        if not source_video_path or not os.path.exists(source_video_path):
            print(f"âš ï¸  è·³è¿‡è§†é¢‘å¤„ç†ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ {source_video_path}")
            return False
        
        output_path = Path(output_dir)
        target_mp4 = output_path / f'{target_name}.mp4'
        target_audio = output_path / f'{target_name}.wav'
        
        print(f"\nğŸ“¹ åˆ†æè§†é¢‘æ–‡ä»¶: {source_video_path}")
        video_info = self.get_video_info(source_video_path)
        if video_info:
            print(f"   æ ¼å¼: {video_info['format']}")
            print(f"   æ—¶é•¿: {video_info['duration']:.2f}ç§’")
            print(f"   å¸§ç‡: {video_info['fps']:.2f} FPS")
            print(f"   åˆ†è¾¨ç‡: {video_info['size'][0]}x{video_info['size'][1]}")
            print(f"   åŒ…å«éŸ³é¢‘: {'æ˜¯' if video_info['has_audio'] else 'å¦'}")
        
        source_ext = os.path.splitext(source_video_path)[1].lower()
        supported_formats = ['.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm']
        
        if source_ext not in supported_formats:
            print(f"âš ï¸  è­¦å‘Š: ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼ {source_ext}")
            print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats)}")
            return False
        
        need_convert = True
        if source_ext == '.mp4':
            print(f"ğŸ“‹ å¤åˆ¶MP4æ–‡ä»¶: {source_video_path} -> {target_mp4}")
            shutil.copy2(source_video_path, target_mp4)
            need_convert = False
            
            try:
                from moviepy.editor import VideoFileClip
                clip = VideoFileClip(source_video_path)
                if clip.audio is not None:
                    print(f"ğŸµ æå–éŸ³é¢‘æ–‡ä»¶...")
                    clip.audio.write_audiofile(str(target_audio), verbose=False, logger=None)
                    print(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {target_audio}")
                else:
                    print("âš ï¸  æºè§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“")
                clip.close()
            except Exception as e:
                print(f"âš ï¸  éŸ³é¢‘æå–å¤±è´¥: {str(e)}")
        
        if need_convert:
            video_success, audio_path = self.convert_video_format(source_video_path, str(target_mp4))
            if not video_success:
                print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {source_video_path}")
                return False
        
        video_exists = target_mp4.exists() and target_mp4.stat().st_size > 0
        audio_exists = target_audio.exists() and target_audio.stat().st_size > 0
        
        print(f"ğŸ“‹ è§†é¢‘æ–‡ä»¶: {'âœ…' if video_exists else 'âŒ'} {target_mp4}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {'âœ…' if audio_exists else 'âŒ'} {target_audio}")
        
        return video_exists
        
    def create_flame_params_json(self, flame_param_dir, output_dir, max_frames):
        """åˆ›å»ºåˆå¹¶çš„flame_params.jsonæ–‡ä»¶"""
        all_params = {
            "expr": [],
            "rotation": [], 
            "neck_pose": [],
            "jaw_pose": [],
            "eyes_pose": [],
            "translation": [],
            "betas": None
        }
        
        for i in range(max_frames):
            flame_file = Path(flame_param_dir) / f"{i:05d}.npz"
            if flame_file.exists():
                data = np.load(flame_file)
                
                for key in ['expr', 'rotation', 'neck_pose', 'jaw_pose', 'eyes_pose', 'translation']:
                    if key in data:
                        if data[key].ndim == 2:
                            all_params[key].append(data[key][0].tolist())
                        else:
                            all_params[key].append(data[key].tolist())
                
                if all_params["betas"] is None and "betas" in data:
                    all_params["betas"] = data["betas"][0].tolist()
        
        json_file = Path(output_dir) / "flame_params.json"
        with open(json_file, 'w') as f:
            json.dump(all_params, f, indent=2)
            
    def convert_vhap_to_lam(self, vhap_dir, output_dir, max_frames=264, target_name="custom_drive", source_video_path=None):
        """è½¬æ¢VHAPè¾“å‡ºä¸ºLAMæ ¼å¼"""
        vhap_path = Path(vhap_dir)
        output_path = Path(output_dir) / target_name
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ”„ è½¬æ¢VHAPæ•°æ®ï¼š{vhap_path} -> {output_path}")
        
        # 1. å¤åˆ¶å¹¶è½¬æ¢flame_paramæ–‡ä»¶
        flame_param_dir = output_path / "flame_param"
        flame_param_dir.mkdir(exist_ok=True)
        
        vhap_flame_dir = vhap_path / "flame_param"
        flame_files = sorted([f for f in os.listdir(vhap_flame_dir) if f.endswith('.npz')])
        
        flame_files = flame_files[:max_frames]
        print(f"ğŸ”¥ å¤„ç†{len(flame_files)}ä¸ªFLAMEå‚æ•°æ–‡ä»¶...")
        
        for i, flame_file in enumerate(flame_files):
            vhap_data = np.load(vhap_flame_dir / flame_file)
            
            lam_data = {}
            
            for key in ['translation', 'rotation', 'neck_pose', 'jaw_pose', 'eyes_pose', 'expr']:
                if key in vhap_data:
                    lam_data[key] = vhap_data[key]
            
            if i == 0 and 'shape' in vhap_data:
                lam_data['betas'] = vhap_data['shape'].reshape(1, -1)
            
            output_file = flame_param_dir / f"{i:05d}.npz"
            np.savez(output_file, **lam_data)
        
        # 2. å¤åˆ¶canonical_flame_param.npz
        canonical_src = vhap_path / "canonical_flame_param.npz"
        canonical_dst = output_path / "canonical_flame_param.npz"
        if canonical_src.exists():
            shutil.copy2(canonical_src, canonical_dst)
            print("ğŸ“‹ å¤åˆ¶canonical_flame_param.npz")
        
        # 3. å¤åˆ¶transformsç›¸å…³æ–‡ä»¶
        for transform_file in ['transforms.json', 'transforms_train.json', 'transforms_test.json', 'transforms_val.json']:
            src_file = vhap_path / transform_file
            dst_file = output_path / transform_file
            if src_file.exists():
                shutil.copy2(src_file, dst_file)
                print(f"ğŸ“‹ å¤åˆ¶{transform_file}")
        
        # 4. åˆ›å»ºflame_params.json
        print("ğŸ“ ç”Ÿæˆflame_params.json...")
        self.create_flame_params_json(flame_param_dir, output_path, max_frames)
        
        # 5. å¤„ç†è§†é¢‘æ–‡ä»¶
        if source_video_path:
            print(f"\nğŸ¬ å¤„ç†è§†é¢‘æ–‡ä»¶...")
            video_success = self.process_video_file(source_video_path, output_path, target_name)
            if video_success:
                print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ")
            else:
                print(f"âš ï¸  è§†é¢‘å¤„ç†å¤±è´¥")
        
        # 6. æœ€ç»ˆçŠ¶æ€æ£€æŸ¥
        print(f"\nğŸ“‹ æœ€ç»ˆæ–‡ä»¶æ£€æŸ¥:")
        required_files = [
            (output_path / f"{target_name}.mp4", "é©±åŠ¨è§†é¢‘"),
            (output_path / f"{target_name}.wav", "éŸ³é¢‘æ–‡ä»¶"),
            (output_path / "flame_param", "FLAMEå‚æ•°ç›®å½•"),
            (output_path / "flame_params.json", "åˆå¹¶çš„FLAMEå‚æ•°"),
            (output_path / "transforms.json", "å˜æ¢çŸ©é˜µ")
        ]
        
        all_ready = True
        for file_path, description in required_files:
            exists = file_path.exists()
            status = "âœ…" if exists else "âŒ"
            print(f"   {status} {description}: {file_path}")
            if not exists:
                all_ready = False
        
        if all_ready:
            print(f"\nğŸ‰ VHAPè½¬æ¢å®Œæˆï¼æ‰€æœ‰å¿…éœ€æ–‡ä»¶å·²å°±ç»ª")
            print(f"   é©±åŠ¨åç§°è®¾ç½®ä¸º: {target_name}")
        else:
            print(f"\nâš ï¸  VHAPè½¬æ¢å®Œæˆï¼Œä½†éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±")
        
        return str(output_path), all_ready
        
    def make_yaw_rotation_matrix(self, yaw_deg, device='cpu', dtype=torch.float32):
        """ç»•ä¸–ç•Œåæ ‡ç³»Yè½´æ—‹è½¬çš„4x4é½æ¬¡çŸ©é˜µ"""
        theta = np.deg2rad(yaw_deg)
        c, s = np.cos(theta), np.sin(theta)
        return torch.tensor([
            [c, 0.0, s, 0.0],
            [0.0, 1.0, 0.0, 0.0],
            [-s, 0.0, c, 0.0],
            [0.0, 0.0, 0.0, 1.0],
        ], device=device, dtype=dtype)
        
    def make_pitch_rotation_matrix(self, pitch_deg, device='cpu', dtype=torch.float32):
        """ç»•ä¸–ç•Œåæ ‡ç³»Xè½´æ—‹è½¬çš„4x4é½æ¬¡çŸ©é˜µ"""
        theta = np.deg2rad(pitch_deg)
        c, s = np.cos(theta), np.sin(theta)
        return torch.tensor([
            [1.0, 0.0, 0.0, 0.0],
            [0.0, c, -s, 0.0],
            [0.0, s, c, 0.0],
            [0.0, 0.0, 0.0, 1.0],
        ], device=device, dtype=dtype)
        
    def save_images2video(self, img_lst, v_pth, fps):
        """ä¿å­˜å›¾åƒåˆ—è¡¨ä¸ºè§†é¢‘"""
        from moviepy.editor import ImageSequenceClip
        images = [image.astype(np.uint8) for image in img_lst]
        clip = ImageSequenceClip(images, fps=fps)
        clip.write_videofile(v_pth, codec='libx264')
        print(f"Video saved successfully at {v_pth}")
        
    def add_audio_to_video(self, video_path, out_path, audio_path):
        """ä¸ºè§†é¢‘æ·»åŠ éŸ³é¢‘"""
        from moviepy.editor import VideoFileClip, AudioFileClip
        
        video_clip = VideoFileClip(video_path)
        video_duration = video_clip.duration
        
        audio_clip = AudioFileClip(audio_path)
        audio_duration = audio_clip.duration
        
        print(f"è§†é¢‘æ—¶é•¿: {video_duration:.2f}ç§’, éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’")
        
        if audio_duration > video_duration:
            audio_clip = audio_clip.subclip(0, video_duration)
            print(f"éŸ³é¢‘è¿‡é•¿ï¼Œæˆªæ–­åˆ° {video_duration:.2f}ç§’")
        elif audio_duration < video_duration:
            audio_clip = audio_clip.loop(duration=video_duration)
            print(f"éŸ³é¢‘è¿‡çŸ­ï¼Œå¾ªç¯æ’­æ”¾åˆ° {video_duration:.2f}ç§’")
        
        audio_clip = audio_clip.set_duration(video_duration)
        video_clip_with_audio = video_clip.set_audio(audio_clip)
        
        video_clip_with_audio.write_videofile(
            out_path, 
            codec='libx264', 
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True
        )
        
        print(f"âœ… éŸ³é¢‘æ·»åŠ æˆåŠŸï¼Œæœ€ç»ˆè§†é¢‘æ—¶é•¿: {video_duration:.2f}ç§’ï¼Œä¿å­˜è‡³: {out_path}")
        
    def generate_multiview_videos(self, image_path, drive_path, output_dir, yaw_angles, pitch_angles):
        """ç”Ÿæˆå¤šè§†è§’å¤´åƒè§†é¢‘"""
        print("=== å¼€å§‹å¤šè§†è§’è§†é¢‘ç”Ÿæˆ ===")
        
        class DummyDir:
            def __init__(self, name):
                self.name = name
        
        working_dir = DummyDir(output_dir)
        
        total_combinations = len(yaw_angles) * len(pitch_angles)
        print(f"å°†ç”Ÿæˆ {total_combinations} ä¸ªä¸åŒè§’åº¦çš„è§†é¢‘")
        print(f"Yawè§’åº¦: {yaw_angles}")
        print(f"Pitchè§’åº¦: {pitch_angles}")
        
        generated_videos = []
        
        for i, yaw in enumerate(yaw_angles):
            for j, pitch in enumerate(pitch_angles):
                current_combo = i * len(pitch_angles) + j + 1
                print(f"\n=== ç”Ÿæˆè§†é¢‘ {current_combo}/{total_combinations}: Yaw={yaw}Â°, Pitch={pitch}Â° ===")
                
                try:
                    img_path, vid_path = self._generate_single_view(
                        image_path, drive_path, working_dir, yaw, pitch
                    )
                    print(f'âœ… å¤„ç†åå›¾ç‰‡ä¿å­˜äº: {img_path}')
                    print(f'âœ… ç”Ÿæˆè§†é¢‘ä¿å­˜äº: {vid_path}')
                    generated_videos.append({
                        'yaw': yaw,
                        'pitch': pitch,
                        'video_path': vid_path,
                        'image_path': img_path
                    })
                except Exception as e:
                    print(f'âŒ ç”Ÿæˆå¤±è´¥ (Yaw={yaw}Â°, Pitch={pitch}Â°): {str(e)}')
                    continue
        
        print(f"\n=== å¤šè§†è§’è§†é¢‘ç”Ÿæˆå®Œæˆ ===")
        print(f"æˆåŠŸç”Ÿæˆ {len(generated_videos)} ä¸ªè§†é¢‘:")
        for video_info in generated_videos:
            print(f"  Yaw={video_info['yaw']:3d}Â°, Pitch={video_info['pitch']:3d}Â° -> {video_info['video_path']}")
            
        return generated_videos
        
    def _generate_single_view(self, image_path, video_params, working_dir, yaw_deg=0.0, pitch_deg=0.0):
        """ç”Ÿæˆå•ä¸ªè§†è§’çš„è§†é¢‘"""
        image_raw = os.path.join(working_dir.name, "raw.png")
        with Image.open(image_path).convert('RGB') as img:
            img.save(image_raw)
        
        base_vid = os.path.basename(video_params).split(".")[0]
        flame_params_dir = os.path.join("./assets/sample_motion/export", base_vid, "flame_param")
        
        dump_image_dir = working_dir.name
        os.makedirs(dump_image_dir, exist_ok=True)
        
        angle_tag = f'_yaw{int(yaw_deg)}_pitch{int(pitch_deg)}'
        dump_video_path = os.path.join(dump_image_dir, f'output{angle_tag}.mp4')
        dump_image_path = os.path.join(dump_image_dir, f'output{angle_tag}.png')
        dump_video_path_wa = os.path.join(dump_image_dir, f'output{angle_tag}_audio.mp4')
        
        # é¢„å¤„ç†è¾“å…¥å›¾åƒ
        return_code = self.flametracking.preprocess(image_raw)
        assert (return_code == 0), "flametracking preprocess failed!"
        return_code = self.flametracking.optimize()
        assert (return_code == 0), "flametracking optimize failed!"
        return_code, output_dir = self.flametracking.export()
        assert (return_code == 0), "flametracking export failed!"
        
        image_path_processed = os.path.join(output_dir, "images/00000_00.png")
        mask_path = image_path_processed.replace("/images/", "/fg_masks/").replace(".jpg", ".png")
        
        aspect_standard = 1.0 / 1.0
        source_size = self.config.source_size
        render_size = self.config.render_size
        render_fps = 30
        
        # å‡†å¤‡å‚è€ƒå›¾åƒ
        image, _, _, shape_param = preprocess_image(
            image_path_processed, mask_path=mask_path, intr=None, pad_ratio=0,
            bg_color=1., max_tgt_size=None, aspect_standard=aspect_standard,
            enlarge_ratio=[1.0, 1.0], render_tgt_size=source_size, multiply=14, 
            need_mask=True, get_shape_param=True
        )
        
        # ä¿å­˜å¤„ç†åçš„å›¾åƒ
        vis_ref_img = (image[0].permute(1, 2, 0).cpu().detach().numpy() * 255).astype(np.uint8)
        Image.fromarray(vis_ref_img).save(dump_image_path)
        
        # å‡†å¤‡è¿åŠ¨åºåˆ—
        src = image_path.split('/')[-3] if len(image_path.split('/')) > 3 else 'default'
        driven = flame_params_dir.split('/')[-2]
        src_driven = [src, driven]
        
        motion_seq = prepare_motion_seqs(
            flame_params_dir, None, save_root=dump_image_dir, fps=render_fps,
            bg_color=1., aspect_standard=aspect_standard, enlarge_ratio=[1.0, 1.0],
            render_image_res=render_size, multiply=16, need_mask=False, vis_motion=False,
            shape_param=shape_param, test_sample=False, cross_id=False,
            src_driven=src_driven
        )
        
        device, dtype = "cuda", torch.float32
        
        # åº”ç”¨æ—‹è½¬å˜æ¢
        orig_c2w = motion_seq["render_c2ws"].to(device)
        pitch_mat = self.make_pitch_rotation_matrix(pitch_deg, device=device)
        yaw_mat = self.make_yaw_rotation_matrix(yaw_deg, device=device)
        R_total = yaw_mat @ pitch_mat
        R_batch = R_total.unsqueeze(0)
        new_c2ws = torch.matmul(R_batch, orig_c2w)
        
        # å¼€å§‹æ¨ç†
        num_views = motion_seq["render_c2ws"].shape[1]
        motion_seq["flame_params"]["betas"] = shape_param.unsqueeze(0)
        
        print("å¼€å§‹æ¨ç†...")
        frames = []
        batch_size = 64
        
        for start in tqdm(range(0, num_views, batch_size), desc='æ¨ç†è¿›åº¦'):
            end = min(start + batch_size, num_views)
            c2ws = new_c2ws[:, start:end, ...].to(device, dtype).contiguous()
            intrs = motion_seq["render_intrs"][:, start:end, ...].to(device, dtype).contiguous()
            bg_colors = motion_seq["render_bg_colors"][:, start:end, ...].to(device, dtype).contiguous()
            
            flame_params_batch = {}
            for k, v in motion_seq["flame_params"].items():
                if k == "betas":
                    chunk = v
                else:
                    chunk = v[:, start:end, ...]
                flame_params_batch[k] = chunk.to(device, dtype).contiguous()
            
            with torch.no_grad():
                image_5d = image.to(device, dtype).unsqueeze(1)
                res = self.lam_model.infer_single_view(
                    image_5d, None, None,
                    render_c2ws=c2ws,
                    render_intrs=intrs,
                    render_bg_colors=bg_colors,
                    flame_params=flame_params_batch,
                )
            
            rgb = res["comp_rgb"].cpu().numpy()
            if rgb.ndim == 5:
                rgb = rgb[0].transpose(0, 2, 3, 1)
            elif rgb.ndim == 4 and rgb.shape[-1] == 3:
                pass
            else:
                raise ValueError(f"Unexpected rgb shape: {rgb.shape}")
            
            rgb = (rgb * 255).astype(np.uint8)
            frames.extend(list(rgb))
        
        print(f'æ”¶é›†åˆ° {len(frames)} å¸§ï¼Œç¬¬ä¸€å¸§å½¢çŠ¶: {frames[0].shape}')
        
        # ä¿å­˜è§†é¢‘
        self.save_images2video(frames, dump_video_path, render_fps)
        
        # æ·»åŠ éŸ³é¢‘
        extracted_audio_path = os.path.join("./assets/sample_motion/export", base_vid, base_vid + ".wav")
        fallback_audio_path = os.path.join("./assets/sample_motion/export", "Joe_Biden", "Joe_Biden.wav")
        
        if os.path.exists(extracted_audio_path) and os.path.getsize(extracted_audio_path) > 0:
            audio_path = extracted_audio_path
            print(f"âœ… ä½¿ç”¨ä»æºè§†é¢‘æå–çš„éŸ³é¢‘: {audio_path}")
        elif os.path.exists(fallback_audio_path):
            audio_path = fallback_audio_path
            print(f"âš ï¸  æºè§†é¢‘éŸ³é¢‘ä¸å¯ç”¨ï¼Œä½¿ç”¨å ä½ç¬¦éŸ³é¢‘: {audio_path}")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶ï¼Œè·³è¿‡éŸ³é¢‘æ·»åŠ ")
            return dump_image_path, dump_video_path
        
        self.add_audio_to_video(dump_video_path, dump_video_path_wa, audio_path)
        
        return dump_image_path, dump_video_path_wa

def parse_configs():
    """è§£æé…ç½®å‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str)
    parser.add_argument("--infer", type=str)
    args, unknown = parser.parse_known_args()

    cfg = OmegaConf.create()
    cli_cfg = OmegaConf.from_cli(unknown)

    # ä»ç¯å¢ƒå˜é‡è§£æ
    if os.environ.get("APP_INFER") is not None:
        args.infer = os.environ.get("APP_INFER")
    if os.environ.get("APP_MODEL_NAME") is not None:
        cli_cfg.model_name = os.environ.get("APP_MODEL_NAME")

    args.config = args.infer if args.config is None else args.config

    if args.config is not None:
        cfg_train = OmegaConf.load(args.config)
        cfg.source_size = cfg_train.dataset.source_image_res
        try:
            cfg.src_head_size = cfg_train.dataset.src_head_size
        except:
            cfg.src_head_size = 112
        cfg.render_size = cfg_train.dataset.render_image.high

    if args.infer is not None:
        cfg_infer = OmegaConf.load(args.infer)
        cfg.merge_with(cfg_infer)

    cfg.motion_video_read_fps = 30
    cfg.merge_with(cli_cfg)
    cfg.setdefault("logger", "INFO")

    return cfg

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•´åˆçš„å¤´åƒç”Ÿæˆç³»ç»Ÿ')
    
    # è¾“å…¥å‚æ•°
    parser.add_argument('--input_image', type=str, required=True,
                       help='è¾“å…¥å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--source_video', type=str,
                       help='æºè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰')
    parser.add_argument('--vhap_dir', type=str,
                       help='VHAPè¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœéœ€è¦è½¬æ¢ï¼‰')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='./output_integrated/',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--drive_name', type=str, default='custom_integrated_drive',
                       help='é©±åŠ¨æ•°æ®åç§°')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_name', type=str, 
                       default="/root/autodl-tmp/LAM/model_zoo/lam_models/releases/lam/lam-20k/step_045500",
                       help='LAMæ¨¡å‹è·¯å¾„')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--yaw_angles', nargs='+', type=int, 
                       default=[-30, -20, -10, 0, 10, 20, 30],
                       help='åèˆªè§’åº¦åˆ—è¡¨')
    parser.add_argument('--pitch_angles', nargs='+', type=int,
                       default=[-10, -5, 0, 5, 10],
                       help='ä¿¯ä»°è§’åº¦åˆ—è¡¨')
    parser.add_argument('--max_frames', type=int, default=264,
                       help='æœ€å¤§å¸§æ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["APP_MODEL_NAME"] = args.model_name
    os.environ["APP_INFER"] = "./configs/inference/lam-20k-8gpu.yaml"
    
    print("ğŸš€ å¯åŠ¨æ•´åˆçš„å¤´åƒç”Ÿæˆç³»ç»Ÿ")
    print(f"   è¾“å…¥å›¾ç‰‡: {args.input_image}")
    print(f"   æºè§†é¢‘: {args.source_video}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   é©±åŠ¨åç§°: {args.drive_name}")
    print(f"   Yawè§’åº¦: {args.yaw_angles}")
    print(f"   Pitchè§’åº¦: {args.pitch_angles}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è§£æé…ç½®
    cfg = parse_configs()
    cfg.model_name = args.model_name
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = IntegratedAvatarGenerator(cfg)
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("\n=== åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶ ===")
    generator.init_preprocessor()
    generator.init_flame_tracking()
    generator.init_lam_model()
    
    # å¦‚æœæä¾›äº†VHAPç›®å½•ï¼Œå…ˆè¿›è¡Œè½¬æ¢
    if args.vhap_dir:
        print("\n=== è½¬æ¢VHAPæ•°æ® ===")
        drive_output_dir = "./assets/sample_motion/export"
        drive_path, conversion_success = generator.convert_vhap_to_lam(
            vhap_dir=args.vhap_dir,
            output_dir=drive_output_dir,
            max_frames=args.max_frames,
            target_name=args.drive_name,
            source_video_path=args.source_video
        )
        
        if not conversion_success:
            print("âŒ VHAPè½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®")
            return
            
        video_path = os.path.join(drive_path, f"{args.drive_name}.mp4")
    else:
        # ç›´æ¥ä½¿ç”¨ç°æœ‰çš„é©±åŠ¨æ•°æ®
        video_path = f"./assets/sample_motion/export/{args.drive_name}/{args.drive_name}.mp4"
        if not os.path.exists(video_path):
            print(f"âŒ é©±åŠ¨è§†é¢‘ä¸å­˜åœ¨: {video_path}")
            print("è¯·æä¾› --vhap_dir è¿›è¡Œæ•°æ®è½¬æ¢ï¼Œæˆ–ç¡®ä¿é©±åŠ¨æ•°æ®å·²å­˜åœ¨")
            return
    
    # ç”Ÿæˆå¤šè§†è§’è§†é¢‘
    print("\n=== ç”Ÿæˆå¤šè§†è§’å¤´åƒè§†é¢‘ ===")
    generated_videos = generator.generate_multiview_videos(
        image_path=args.input_image,
        drive_path=video_path,
        output_dir=args.output_dir,
        yaw_angles=args.yaw_angles,
        pitch_angles=args.pitch_angles
    )
    
    print(f"\nğŸ‰ æ•´åˆä»»åŠ¡å®Œæˆï¼")
    print(f"   ç”Ÿæˆäº† {len(generated_videos)} ä¸ªå¤šè§†è§’è§†é¢‘")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")

if __name__ == '__main__':
    main() 